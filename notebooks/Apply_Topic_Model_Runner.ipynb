{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Apply Topic Model to New Documents\n\nThis notebook makes it easy to apply a previously trained topic model to new documents.\n\n**Use this when:**\n- You have already trained a topic model\n- You want to classify new documents using that model\n- You want to see topic distributions for incoming data\n\n**How to use:**\n1. Edit the configuration in Section 1 (paths to your trained model and new data)\n2. **Run Section 2 first** to install all prerequisites (one-time setup)\n3. Run all remaining cells\n4. View topic assignments and export results\n\n**First time users:** Make sure to run Section 2 (Install Prerequisites) before running the rest of the notebook!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINED MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Path to your trained model directory\n",
    "MODEL_DIR = 'results/topic_modeling_20240216_140530'  # Change this to your model directory\n",
    "\n",
    "# Model files (usually you don't need to change these)\n",
    "MODEL_FILE = 'lda_model'\n",
    "DICTIONARY_FILE = 'lda_dictionary'\n",
    "BIGRAM_FILE = 'lda_bigram.pkl'  # Optional\n",
    "\n",
    "# ============================================================================\n",
    "# NEW DOCUMENTS CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Choose input type: 'csv' or 'directory'\n",
    "INPUT_TYPE = 'csv'\n",
    "\n",
    "# For CSV input\n",
    "INPUT_CSV = 'Data/new_documents.csv'\n",
    "TEXT_COLUMN = 'content'\n",
    "\n",
    "# For directory input\n",
    "INPUT_DIRECTORY = 'Data/new_documents/'\n",
    "FILE_PATTERN = '*.txt'\n",
    "\n",
    "# ============================================================================\n",
    "# OUTPUT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Where to save topic assignments\n",
    "OUTPUT_CSV = 'results/topic_assignments.csv'\n",
    "\n",
    "# Include probabilities for ALL topics (not just the dominant one)\n",
    "INCLUDE_ALL_TOPICS = True\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING PARAMETERS\n",
    "# ============================================================================\n",
    "# These should match what you used when training the model\n",
    "\n",
    "ALLOWED_POS = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "CUSTOM_STOPWORDS = []  # Should match training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Install Prerequisites\n\nThis section installs all required packages. **Run this cell first** if you get \"module not found\" errors.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nimport subprocess\n\nprint(\"Installing required packages...\\n\")\nprint(\"=\"*60)\n\n# List of required packages\npackages = [\n    'gensim',\n    'pandas',\n    'numpy',\n    'nltk',\n    'spacy',\n    'scikit-learn',\n    'matplotlib'\n]\n\n# Install packages\nfor package in packages:\n    print(f\"Installing {package}...\")\n    try:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n        print(f\"  ✓ {package} installed\")\n    except subprocess.CalledProcessError as e:\n        print(f\"  ✗ Error installing {package}: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Installing spaCy language model...\")\nprint(\"=\"*60)\n\n# Install spaCy model\ntry:\n    import spacy\n    try:\n        nlp = spacy.load('en_core_web_lg')\n        print(\"✓ en_core_web_lg already installed\")\n    except OSError:\n        print(\"Downloading en_core_web_lg (this may take a few minutes)...\")\n        subprocess.check_call([sys.executable, '-m', 'spacy', 'download', 'en_core_web_lg'])\n        print(\"✓ en_core_web_lg installed\")\nexcept Exception as e:\n    print(f\"Note: If this fails, you can use the smaller model.\")\n    print(f\"Run: python -m spacy download en_core_web_sm\")\n    print(f\"Error: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Downloading NLTK data...\")\nprint(\"=\"*60)\n\n# Download NLTK data\ntry:\n    import nltk\n    nltk.download('stopwords', quiet=True)\n    nltk.download('punkt', quiet=True)\n    print(\"✓ NLTK data downloaded\")\nexcept Exception as e:\n    print(f\"✗ Error downloading NLTK data: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✓ All prerequisites installed!\")\nprint(\"=\"*60)\nprint(\"\\nYou can now proceed to the next sections.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"Checking configuration...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check model files\n",
    "model_dir_path = project_root / MODEL_DIR\n",
    "model_path = model_dir_path / MODEL_FILE\n",
    "dict_path = model_dir_path / DICTIONARY_FILE\n",
    "bigram_path = model_dir_path / BIGRAM_FILE\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f\"✓ Model found: {model_path}\")\n",
    "else:\n",
    "    print(f\"✗ Model not found: {model_path}\")\n",
    "\n",
    "if dict_path.exists():\n",
    "    print(f\"✓ Dictionary found: {dict_path}\")\n",
    "else:\n",
    "    print(f\"✗ Dictionary not found: {dict_path}\")\n",
    "\n",
    "if bigram_path.exists():\n",
    "    print(f\"✓ Bigram model found: {bigram_path}\")\n",
    "else:\n",
    "    print(f\"  Bigram model not found (optional): {bigram_path}\")\n",
    "\n",
    "# Check input data\n",
    "print()\n",
    "if INPUT_TYPE == 'csv':\n",
    "    input_path = project_root / INPUT_CSV\n",
    "    if input_path.exists():\n",
    "        print(f\"✓ Input CSV found: {input_path}\")\n",
    "    else:\n",
    "        print(f\"✗ Input CSV not found: {input_path}\")\n",
    "else:\n",
    "    input_dir = project_root / INPUT_DIRECTORY\n",
    "    if input_dir.exists():\n",
    "        files = list(input_dir.glob(FILE_PATTERN))\n",
    "        print(f\"✓ Input directory found: {input_dir}\")\n",
    "        print(f\"  Found {len(files)} files matching '{FILE_PATTERN}'\")\n",
    "    else:\n",
    "        print(f\"✗ Input directory not found: {input_dir}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4: Load Model Info"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Try to load model configuration if available\n",
    "config_path = model_dir_path / 'run_configuration.json'\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        model_config = json.load(f)\n",
    "    print(\"Model training configuration:\")\n",
    "    print(json.dumps(model_config, indent=2))\n",
    "else:\n",
    "    print(\"No configuration file found\")\n",
    "\n",
    "# Load topics info\n",
    "topics_json = model_dir_path / 'lda_topics.json'\n",
    "if topics_json.exists():\n",
    "    with open(topics_json, 'r') as f:\n",
    "        topics_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n\\nModel has {len(topics_data)} topics:\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for topic_id, topic_info in sorted(topics_data.items()):\n",
    "        topic_num = topic_id.split('_')[1]\n",
    "        top_words = \", \".join([w['word'] for w in topic_info['words'][:5]])\n",
    "        print(f\"Topic {topic_num}: {top_words}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\nTopics file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: Build and Run Command"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Build command\n",
    "script_path = project_root / 'scripts' / 'apply_topic_model.py'\n",
    "\n",
    "cmd = ['python', str(script_path)]\n",
    "\n",
    "# Add model paths\n",
    "cmd.extend(['--model', str(model_path)])\n",
    "cmd.extend(['--dictionary', str(dict_path)])\n",
    "if bigram_path.exists():\n",
    "    cmd.extend(['--bigram', str(bigram_path)])\n",
    "\n",
    "# Add input\n",
    "if INPUT_TYPE == 'csv':\n",
    "    cmd.extend(['--input', str(project_root / INPUT_CSV)])\n",
    "    cmd.extend(['--text-col', TEXT_COLUMN])\n",
    "else:\n",
    "    cmd.extend(['--input-dir', str(project_root / INPUT_DIRECTORY)])\n",
    "    cmd.extend(['--file-pattern', FILE_PATTERN])\n",
    "\n",
    "# Add output\n",
    "output_path = project_root / OUTPUT_CSV\n",
    "cmd.extend(['--output', str(output_path)])\n",
    "\n",
    "# Add options\n",
    "if INCLUDE_ALL_TOPICS:\n",
    "    cmd.append('--include-all-topics')\n",
    "\n",
    "if ALLOWED_POS:\n",
    "    cmd.extend(['--allowed-postags'] + ALLOWED_POS)\n",
    "\n",
    "if CUSTOM_STOPWORDS:\n",
    "    cmd.extend(['--custom-stopwords'] + CUSTOM_STOPWORDS)\n",
    "\n",
    "# Display command\n",
    "print(\"Command:\")\n",
    "print(\" \".join(cmd))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Run command\n",
    "print(\"\\nApplying model to new documents...\\n\")\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if process.returncode == 0:\n",
    "    print(\"✓ Topic assignment completed successfully!\")\n",
    "else:\n",
    "    print(f\"✗ Error occurred (exit code: {process.returncode})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 6: View Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if output_path.exists():\n",
    "    results_df = pd.read_csv(output_path)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(results_df)} document assignments\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nFirst 10 documents:\")\n",
    "    display(results_df.head(10))\n",
    "    \n",
    "    # Show topic distribution\n",
    "    print(\"\\n\\nTopic Distribution:\")\n",
    "    topic_counts = results_df['dominant_topic'].value_counts().sort_index()\n",
    "    \n",
    "    topic_dist = []\n",
    "    for topic_id, count in topic_counts.items():\n",
    "        if topic_id >= 0:  # Skip -1 (failed docs)\n",
    "            keywords = results_df[results_df['dominant_topic'] == topic_id]['topic_keywords'].iloc[0]\n",
    "            topic_dist.append({\n",
    "                'Topic': topic_id,\n",
    "                'Documents': count,\n",
    "                'Percentage': f\"{count/len(results_df)*100:.1f}%\",\n",
    "                'Keywords': keywords\n",
    "            })\n",
    "    \n",
    "    topic_dist_df = pd.DataFrame(topic_dist)\n",
    "    display(topic_dist_df)\n",
    "    \n",
    "else:\n",
    "    print(f\"Output file not found: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 7: Visualize Topic Distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'results_df' in locals():\n",
    "    # Bar chart of topic distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    topic_counts = results_df[results_df['dominant_topic'] >= 0]['dominant_topic'].value_counts().sort_index()\n",
    "    \n",
    "    ax.bar(topic_counts.index, topic_counts.values)\n",
    "    ax.set_xlabel('Topic ID')\n",
    "    ax.set_ylabel('Number of Documents')\n",
    "    ax.set_title('Topic Distribution in New Documents')\n",
    "    ax.set_xticks(topic_counts.index)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Pie chart\n",
    "    if len(topic_counts) <= 20:  # Only show pie chart for reasonable number of topics\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.pie(topic_counts.values, labels=[f'Topic {i}' for i in topic_counts.index], autopct='%1.1f%%')\n",
    "        ax.set_title('Topic Distribution (Percentage)')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 8: Analyze Specific Documents"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in locals():\n",
    "    # Find documents with highest topic probability\n",
    "    print(\"Documents with highest topic confidence:\\n\")\n",
    "    top_confident = results_df.nlargest(10, 'topic_prob')[['doc_id', 'dominant_topic', 'topic_prob', 'topic_keywords']]\n",
    "    display(top_confident)\n",
    "    \n",
    "    # If original data is available, show the actual text\n",
    "    if INPUT_TYPE == 'csv' and (project_root / INPUT_CSV).exists():\n",
    "        original_df = pd.read_csv(project_root / INPUT_CSV)\n",
    "        \n",
    "        print(\"\\n\\nSample document from each topic:\\n\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for topic_id in sorted(results_df['dominant_topic'].unique()):\n",
    "            if topic_id >= 0:\n",
    "                # Get highest confidence document for this topic\n",
    "                topic_docs = results_df[results_df['dominant_topic'] == topic_id]\n",
    "                if len(topic_docs) > 0:\n",
    "                    best_doc = topic_docs.nlargest(1, 'topic_prob')\n",
    "                    doc_idx = best_doc['doc_id'].iloc[0]\n",
    "                    prob = best_doc['topic_prob'].iloc[0]\n",
    "                    keywords = best_doc['topic_keywords'].iloc[0]\n",
    "                    \n",
    "                    if doc_idx < len(original_df):\n",
    "                        text = original_df[TEXT_COLUMN].iloc[doc_idx]\n",
    "                        text_preview = text[:200] + \"...\" if len(text) > 200 else text\n",
    "                        \n",
    "                        print(f\"\\nTopic {topic_id} ({keywords})\")\n",
    "                        print(f\"Probability: {prob:.3f}\")\n",
    "                        print(f\"Text: {text_preview}\")\n",
    "                        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 9: Export Summary Report"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if 'results_df' in locals():\n",
    "    # Create summary report\n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_dir': str(MODEL_DIR),\n",
    "        'input_source': str(INPUT_CSV if INPUT_TYPE == 'csv' else INPUT_DIRECTORY),\n",
    "        'total_documents': len(results_df),\n",
    "        'successfully_classified': len(results_df[results_df['dominant_topic'] >= 0]),\n",
    "        'topic_distribution': topic_dist_df.to_dict('records') if 'topic_dist_df' in locals() else [],\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = output_path.parent / f\"{output_path.stem}_report.json\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"Report saved to: {report_path}\")\n",
    "    print(\"\\nSummary:\")\n",
    "    print(json.dumps(report, indent=2))\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Your topic assignments have been saved to: **`{OUTPUT_CSV}`**\n",
    "\n",
    "### Output columns:\n",
    "- `dominant_topic`: The most likely topic for this document\n",
    "- `topic_prob`: Confidence/probability of the dominant topic\n",
    "- `topic_keywords`: Top keywords for the dominant topic\n",
    "- `topic_X_prob`: Probability for each topic (if INCLUDE_ALL_TOPICS = True)\n",
    "\n",
    "### Next steps:\n",
    "- Use the results CSV for further analysis\n",
    "- Filter documents by topic\n",
    "- Integrate topic assignments into your workflow\n",
    "- Apply the model to additional new documents as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}