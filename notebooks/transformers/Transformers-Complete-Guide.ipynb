{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Complete Guide\n",
    "\n",
    "**Summary:**\n",
    "This notebook provides a comprehensive guide to transformer models (BERT, GPT, etc.) for text analysis, including practical examples, comparisons, and applications in literary studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Complete Guide\n",
    "\n",
    "This comprehensive notebook covers transformer models using the [Hugging Face Transformers](https://huggingface.co/docs/transformers/) library, from basic pre-trained pipelines to fine-tuning for custom classification tasks.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Text generation with GPT-2\n",
    "2. Text embeddings and semantic similarity with BERT\n",
    "3. Pre-trained pipelines (sentiment analysis, NER, zero-shot)\n",
    "4. Fine-tuning BERT for classification tasks\n",
    "5. Advanced zero-shot prompting with FLAN-T5\n",
    "\n",
    "**Authors:** Maria Antoniak, Melanie Walsh, and the [AI for Humanists](https://aiforhumanists.com/) Team  \n",
    "**Updated:** 2025-02-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction & Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook requires the following packages:\n",
    "```bash\n",
    "pip install transformers torch torchvision torchaudio\n",
    "pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "pip install requests gdown sentencepiece\n",
    "```\n",
    "\n",
    "### What are Transformers?\n",
    "\n",
    "Transformer models use self-attention mechanisms to process text, enabling them to:\n",
    "- Understand context and relationships between words\n",
    "- Generate coherent, contextually-appropriate text\n",
    "- Transfer learning from large pre-trained models to specific tasks\n",
    "\n",
    "**Popular transformer models:**\n",
    "- **BERT** (Bidirectional Encoder Representations): Best for understanding text (classification, NER, Q&A)\n",
    "- **GPT** (Generative Pre-trained Transformer): Best for generating text\n",
    "- **T5** (Text-to-Text Transfer Transformer): Frames all NLP tasks as text generation\n",
    "- **DistilBERT**: Smaller, faster version of BERT with ~95% of its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install required packages\n",
    "# !pip install transformers torch scikit-learn pandas numpy matplotlib seaborn requests gdown sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python modules\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# For downloading files\n",
    "import requests\n",
    "try:\n",
    "    import gdown\n",
    "except ImportError:\n",
    "    print(\"Warning: gdown not installed. Some dataset downloads may not work.\")\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning and evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Transformers\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    BertModel, BertTokenizer,\n",
    "    DistilBertTokenizerFast, DistilBertForSequenceClassification,\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "\n",
    "# Progress bars and display\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Setup (CUDA/MPS/CPU)\n",
    "\n",
    "This notebook automatically detects your available hardware:\n",
    "- **CUDA**: NVIDIA GPUs (fastest)\n",
    "- **MPS**: Apple Silicon GPUs (M1/M2/M3)\n",
    "- **CPU**: Fallback (slowest)\n",
    "\n",
    "Fine-tuning on CPU can be very slow. If you don't have GPU access, consider:\n",
    "- Using Google Colab (free GPU)\n",
    "- Reducing dataset size\n",
    "- Using smaller models (DistilBERT instead of BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using CUDA GPU: {device_name}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU (this will be slow for fine-tuning)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Text Generation with GPT-2\n",
    "\n",
    "GPT-2 is an autoregressive language model that generates text by predicting the next token. It was trained on a large corpus of internet text and can generate surprisingly coherent and contextually appropriate text.\n",
    "\n",
    "**Available GPT-2 models:**\n",
    "\n",
    "| Model | Parameters | HF Name |\n",
    "|-------|-----------|----------|\n",
    "| Small | 124M | `gpt2` |\n",
    "| Medium | 355M | `gpt2-medium` |\n",
    "| Large | 774M | `gpt2-large` |\n",
    "| XL | 1.5B | `gpt2-xl` |\n",
    "\n",
    "We'll start with the small model for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "print(f\"Loaded GPT-2 ({gpt2_model.num_parameters():,} parameters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Parameters\n",
    "\n",
    "Understanding these parameters helps you control the quality and creativity of generated text:\n",
    "\n",
    "- **max_new_tokens**: Maximum number of tokens to generate\n",
    "- **temperature**: Controls randomness (0.3 = focused/deterministic, 1.0+ = creative/random)\n",
    "- **top_k**: Only consider the k most likely next tokens\n",
    "- **top_p**: Nucleus sampling — only consider tokens whose cumulative probability reaches p\n",
    "- **num_samples**: How many different completions to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7,\n",
    "             top_k=50, top_p=0.9, num_samples=1):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_samples,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    for i, output in enumerate(outputs):\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        if num_samples > 1:\n",
    "            print(f\"--- Sample {i + 1} ---\")\n",
    "        print(text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation\n",
    "generate(gpt2_model, gpt2_tokenizer, \"The secret of life is\", max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple creative samples\n",
    "generate(gpt2_model, gpt2_tokenizer, \"Once upon a time\",\n",
    "         max_new_tokens=80, temperature=0.9, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own prompts!\n",
    "# Experiment with different temperature values:\n",
    "# - Low (0.3-0.5): More focused, coherent, deterministic\n",
    "# - Medium (0.7-0.8): Balanced creativity and coherence\n",
    "# - High (0.9-1.2): More creative, unpredictable\n",
    "\n",
    "generate(gpt2_model, gpt2_tokenizer, \"In the future, artificial intelligence will\",\n",
    "         max_new_tokens=100, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Text Embeddings with BERT\n",
    "\n",
    "BERT produces **contextual embeddings** — vector representations where the meaning of a word depends on its surrounding context. This is in contrast to older word embeddings (like Word2Vec) where each word has a single fixed vector.\n",
    "\n",
    "**Example:** The word \"bank\" has different meanings in:\n",
    "- \"I deposited money at the **bank**\" (financial institution)\n",
    "- \"We sat by the river **bank**\" (land alongside water)\n",
    "\n",
    "BERT will produce different embeddings for \"bank\" in each context.\n",
    "\n",
    "### The [CLS] Token\n",
    "\n",
    "BERT adds special tokens to the input:\n",
    "- `[CLS]`: Start of sequence token — its embedding is used to represent the entire sequence\n",
    "- `[SEP]`: Separator between sentences\n",
    "- `[PAD]`: Padding to make all sequences the same length\n",
    "\n",
    "For classification and similarity tasks, we use the `[CLS]` token's embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "print(f\"Loaded BERT ({bert_model.num_parameters():,} parameters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"Get [CLS] token embeddings for a list of texts.\"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # [CLS] token is at position 0\n",
    "    return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences with different semantic meanings\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A kitten rested on the rug.\",\n",
    "    \"Stock prices rose sharply today.\",\n",
    "    \"The financial markets surged.\",\n",
    "]\n",
    "\n",
    "embeddings = get_embeddings(sentences, bert_tokenizer, bert_model)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"(batch_size={embeddings.shape[0]}, hidden_size={embeddings.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity with Cosine Distance\n",
    "\n",
    "Cosine similarity measures how similar two vectors are, ranging from -1 to 1:\n",
    "- **1.0**: Identical vectors (same direction)\n",
    "- **0.0**: Orthogonal vectors (no similarity)\n",
    "- **-1.0**: Opposite vectors\n",
    "\n",
    "For text embeddings, higher cosine similarity indicates more semantically similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise cosine similarity\n",
    "n = len(sentences)\n",
    "sim_matrix = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sim_matrix[i, j] = cosine_similarity(embeddings[i].unsqueeze(0), embeddings[j].unsqueeze(0))\n",
    "\n",
    "# Create a nice labeled dataframe\n",
    "labels = [s[:30] + \"...\" if len(s) > 30 else s for s in sentences]\n",
    "sim_df = pd.DataFrame(sim_matrix.cpu().numpy(), index=labels, columns=labels)\n",
    "sim_df.style.background_gradient(cmap=\"YlOrRd\", vmin=0.8, vmax=1.0).format(\"{:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Sentences 1 and 2 (about cats) should have high similarity\n",
    "- Sentences 3 and 4 (about finance) should have high similarity\n",
    "- Cross-topic pairs should have lower similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sim_df, annot=True, fmt=\".3f\", cmap=\"YlOrRd\", vmin=0.8, vmax=1.0)\n",
    "plt.title(\"Semantic Similarity Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Pre-trained Pipelines\n",
    "\n",
    "Hugging Face provides a high-level `pipeline` API for common NLP tasks. These pipelines use pre-trained models fine-tuned for specific tasks, so you can use them immediately without any training.\n",
    "\n",
    "**Available pipelines:**\n",
    "- Sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Zero-shot classification\n",
    "- Question answering\n",
    "- Summarization\n",
    "- Translation\n",
    "- And many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Classifies text as positive or negative. The default model is DistilBERT fine-tuned on movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentiment analysis pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "reviews = [\n",
    "    \"This movie was absolutely wonderful! The acting was superb.\",\n",
    "    \"Terrible film. I walked out after 30 minutes.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"A masterpiece of modern cinema. Truly breathtaking.\",\n",
    "    \"The plot made no sense and the dialogue was awful.\",\n",
    "]\n",
    "\n",
    "results = sentiment(reviews)\n",
    "\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"{result['label']:8} ({result['score']:.3f})  {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Identifies and classifies named entities in text:\n",
    "- **PER**: Person\n",
    "- **ORG**: Organization\n",
    "- **LOC**: Location\n",
    "- **MISC**: Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\", device=0 if device == \"cuda\" else -1)\n",
    "entities = ner(\"Barack Obama graduated from Harvard Law School and served as President of the United States.\")\n",
    "\n",
    "for ent in entities:\n",
    "    print(f\"{ent['entity_group']:10} {ent['word']:25} (score: {ent['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification\n",
    "\n",
    "Classify text into categories **without any training examples**. You just provide:\n",
    "1. The text to classify\n",
    "2. A list of candidate labels\n",
    "\n",
    "The model uses natural language inference to determine which label best fits the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "result = classifier(\n",
    "    \"The new iPhone features a faster processor and improved camera system.\",\n",
    "    candidate_labels=[\"technology\", \"politics\", \"sports\", \"science\"]\n",
    ")\n",
    "\n",
    "for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "    print(f\"{label:15} {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try zero-shot classification on your own text!\n",
    "# Example: classify news headlines, tweets, or book reviews\n",
    "\n",
    "texts = [\n",
    "    \"The Federal Reserve announced an interest rate cut.\",\n",
    "    \"Scientists discovered a new species in the Amazon rainforest.\",\n",
    "    \"The Lakers won in overtime against the Celtics.\",\n",
    "]\n",
    "\n",
    "labels = [\"business\", \"science\", \"sports\", \"entertainment\"]\n",
    "\n",
    "for text in texts:\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Top prediction: {result['labels'][0]} ({result['scores'][0]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Fine-Tuning for Classification\n",
    "\n",
    "While pre-trained pipelines are powerful, you often need to fine-tune a model on your specific dataset. This section demonstrates fine-tuning DistilBERT to classify Goodreads book reviews by genre.\n",
    "\n",
    "**The fine-tuning process:**\n",
    "1. Download and prepare the dataset\n",
    "2. Split into training and test sets\n",
    "3. Encode texts for BERT (tokenization, padding, special tokens)\n",
    "4. Create PyTorch datasets\n",
    "5. Load pre-trained model\n",
    "6. Fine-tune on training data\n",
    "7. Evaluate on test data\n",
    "8. Analyze results and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Goodreads Reviews by Genre\n",
    "\n",
    "We'll use the [UCSD Book Graph dataset](https://mengtingwan.github.io/data/goodreads.html) which contains millions of book reviews. We'll classify reviews into genres:\n",
    "\n",
    "- poetry\n",
    "- comics & graphic\n",
    "- fantasy & paranormal\n",
    "- history & biography\n",
    "- mystery, thriller, & crime\n",
    "- romance\n",
    "- young adult\n",
    "- children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for Goodreads review data by genre\n",
    "genre_url_dict = {\n",
    "    'poetry':                 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_poetry.json.gz',\n",
    "    'children':               'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_children.json.gz',\n",
    "    'comics_graphic':         'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_comics_graphic.json.gz',\n",
    "    'fantasy_paranormal':     'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_fantasy_paranormal.json.gz',\n",
    "    'history_biography':      'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_history_biography.json.gz',\n",
    "    'mystery_thriller_crime': 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_mystery_thriller_crime.json.gz',\n",
    "    'romance':                'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_romance.json.gz',\n",
    "    'young_adult':            'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/goodreads_reviews_young_adult.json.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Sample Data\n",
    "\n",
    "We'll stream the data to avoid downloading huge files. For each genre:\n",
    "1. Stream the first 10,000 reviews\n",
    "2. Randomly sample 2,000 reviews\n",
    "\n",
    "This gives us a manageable dataset while maintaining diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(url, head=10000, sample_size=2000):\n",
    "    \"\"\"Stream reviews from URL and collect a subset.\"\"\"\n",
    "    reviews = []\n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with gzip.open(response.raw, 'rt', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    d = json.loads(line)\n",
    "                    if 'review_text' in d:\n",
    "                        reviews.append(d['review_text'])\n",
    "                        count += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                \n",
    "                if head is not None and count >= head:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from {url}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Return random sample of reviews\n",
    "    return random.sample(reviews, min(sample_size, len(reviews)))\n",
    "\n",
    "# Load reviews for each genre\n",
    "genre_reviews_dict = {}\n",
    "\n",
    "for genre, url in genre_url_dict.items():\n",
    "    print(f'Loading reviews for genre: {genre}')\n",
    "    genre_reviews_dict[genre] = load_reviews(url, head=10000, sample_size=2000)\n",
    "    print(f\"  Loaded {len(genre_reviews_dict[genre])} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a random review from each genre\n",
    "for genre, reviews in genre_reviews_dict.items():\n",
    "    if reviews:\n",
    "        print(f\"\\n{genre.upper()}:\")\n",
    "        print(random.choice(reviews)[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "pickle.dump(genre_reviews_dict, open('genre_reviews_dict.pickle', 'wb'))\n",
    "print(\"Saved genre_reviews_dict.pickle\")\n",
    "\n",
    "# To reload later:\n",
    "# genre_reviews_dict = pickle.load(open('genre_reviews_dict.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Test Sets\n",
    "\n",
    "**Important:** When training machine learning models, we MUST split data into:\n",
    "- **Training set**: Used to train the model (80% of data)\n",
    "- **Test set**: Used to evaluate performance on unseen data (20% of data)\n",
    "\n",
    "**Never** train and test on the same data — this would give falsely high accuracy!\n",
    "\n",
    "For production systems, you should also have a **validation set** for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "for genre, reviews in genre_reviews_dict.items():\n",
    "    # Sample 1000 reviews per genre for this example\n",
    "    reviews = random.sample(reviews, min(1000, len(reviews)))\n",
    "    \n",
    "    # 80/20 split\n",
    "    split_idx = int(len(reviews) * 0.8)\n",
    "    \n",
    "    for review in reviews[:split_idx]:\n",
    "        train_texts.append(review)\n",
    "        train_labels.append(genre)\n",
    "    \n",
    "    for review in reviews[split_idx:]:\n",
    "        test_texts.append(review)\n",
    "        test_labels.append(genre)\n",
    "\n",
    "print(f\"Training samples: {len(train_texts):,}\")\n",
    "print(f\"Test samples: {len(test_texts):,}\")\n",
    "print(f\"\\nExample training sample:\")\n",
    "print(f\"Label: {train_labels[0]}\")\n",
    "print(f\"Text: {train_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Logistic Regression with TF-IDF\n",
    "\n",
    "Before using BERT, let's establish a baseline with a simpler model:\n",
    "- **TF-IDF**: Represents text as weighted word frequencies\n",
    "- **Logistic Regression**: Simple, fast classifier\n",
    "\n",
    "This baseline helps us understand:\n",
    "1. How difficult the classification task is\n",
    "2. Whether BERT provides improvement over simpler methods\n",
    "3. If our data is good quality\n",
    "\n",
    "**Random baseline:** With 8 genres, random guessing would give ~12.5% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Training baseline model (TF-IDF + Logistic Regression)...\")\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# Train logistic regression\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, train_labels)\n",
    "\n",
    "# Evaluate\n",
    "baseline_predictions = baseline_model.predict(X_test)\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(classification_report(test_labels, baseline_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the baseline:**\n",
    "- **Precision**: Of all predictions for a genre, what % were correct?\n",
    "- **Recall**: Of all true instances of a genre, what % did we find?\n",
    "- **F1-score**: Harmonic mean of precision and recall\n",
    "- **Support**: Number of true instances of each class\n",
    "\n",
    "If baseline accuracy > 50%, the task is learnable. Let's see if BERT can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Data for BERT\n",
    "\n",
    "BERT requires text to be processed in a specific way:\n",
    "\n",
    "1. **Tokenization**: Split text into subword tokens (\"running\" → \"run\", \"##ning\")\n",
    "2. **Special tokens**: Add `[CLS]`, `[SEP]`, and `[PAD]`\n",
    "3. **Truncation**: Limit to 512 tokens (BERT's maximum)\n",
    "4. **Padding**: Add `[PAD]` tokens to make all sequences the same length\n",
    "5. **Label encoding**: Convert genre names to integers\n",
    "\n",
    "Fortunately, Hugging Face handles most of this automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokens in BERT\n",
    "\n",
    "| Token | Purpose |\n",
    "|-------|----------|\n",
    "| `[CLS]` | Classification token — placed at start, its embedding represents the whole sequence |\n",
    "| `[SEP]` | Separator — marks boundaries between sentences |\n",
    "| `[PAD]` | Padding — fills sequences to the same length |\n",
    "| `##` | Word piece continuation — indicates this token continues the previous word |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning parameters\n",
    "model_name = 'distilbert-base-cased'\n",
    "max_length = 512\n",
    "cached_model_directory_name = 'distilbert-reviews-genres'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "print(f\"Loaded tokenizer: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mappings\n",
    "unique_labels = sorted(set(train_labels))\n",
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "print(\"Label mappings:\")\n",
    "for label, id in label2id.items():\n",
    "    print(f\"  {id}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode texts\n",
    "print(\"Encoding training data...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "print(\"Encoding test data...\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Encode labels\n",
    "train_labels_encoded = [label2id[y] for y in train_labels]\n",
    "test_labels_encoded = [label2id[y] for y in test_labels]\n",
    "\n",
    "print(\"\\nEncoding complete!\")\n",
    "print(f\"Training samples: {len(train_encodings['input_ids'])}\")\n",
    "print(f\"Test samples: {len(test_encodings['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine an encoded example\n",
    "print(\"Example of BERT tokenization:\")\n",
    "print(\"\\nOriginal text:\")\n",
    "print(train_texts[0][:200])\n",
    "print(\"\\nTokenized (first 50 tokens):\")\n",
    "print(' '.join(train_encodings.tokens(0)[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Datasets\n",
    "\n",
    "PyTorch uses `Dataset` objects to handle data loading and batching. We'll create a custom dataset that:\n",
    "1. Stores our encoded texts and labels\n",
    "2. Returns individual examples in the format BERT expects\n",
    "3. Handles conversion to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewDataset(train_encodings, train_labels_encoded)\n",
    "test_dataset = ReviewDataset(test_encodings, test_labels_encoded)\n",
    "\n",
    "print(f\"Created training dataset with {len(train_dataset)} examples\")\n",
    "print(f\"Created test dataset with {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model\n",
    "\n",
    "We'll load DistilBERT for sequence classification. The model has:\n",
    "- Pre-trained weights from general language modeling\n",
    "- A classification head (randomly initialized) for our 8 genres\n",
    "\n",
    "**Why DistilBERT?**\n",
    "- 40% smaller than BERT\n",
    "- 60% faster\n",
    "- Retains 95% of BERT's performance\n",
    "- Perfect for learning and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DistilBERT for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(id2label)\n",
    ").to(device)\n",
    "\n",
    "print(f\"Loaded {model_name} with {len(id2label)} output classes\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Arguments\n",
    "\n",
    "These hyperparameters control the fine-tuning process. For your own projects, you should experiment with these values:\n",
    "\n",
    "| Parameter | Purpose | Typical Range |\n",
    "|-----------|---------|---------------|\n",
    "| `num_train_epochs` | How many times to iterate through full dataset | 2-5 |\n",
    "| `per_device_train_batch_size` | Training examples per GPU batch | 8-32 |\n",
    "| `learning_rate` | Step size for weight updates | 2e-5 to 5e-5 |\n",
    "| `warmup_steps` | Gradual learning rate increase at start | 100-1000 |\n",
    "| `weight_decay` | Regularization to prevent overfitting | 0.01-0.1 |\n",
    "| `logging_steps` | How often to print progress | 50-500 |\n",
    "\n",
    "**Warning:** Training on CPU can take hours! Use a GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=20,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[],  # Disable wandb logging\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Calculate accuracy from predictions.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model\n",
    "\n",
    "Now we create a `Trainer` object and start fine-tuning!\n",
    "\n",
    "The Trainer will:\n",
    "1. Feed batches of training data through the model\n",
    "2. Calculate loss (how wrong the predictions are)\n",
    "3. Update model weights to reduce loss\n",
    "4. Periodically evaluate on test data\n",
    "5. Log progress\n",
    "\n",
    "**What to watch for:**\n",
    "- **Training loss** should decrease steadily\n",
    "- **Eval accuracy** should increase\n",
    "- If loss plateaus or accuracy stops improving, training is complete\n",
    "- If loss increases or accuracy decreases, you may be overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable wandb logging\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take several minutes (or hours on CPU).\")\n",
    "print(\"Watch for decreasing loss and increasing accuracy.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fine-Tuned Model\n",
    "\n",
    "Save your fine-tuned model so you can use it later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(cached_model_directory_name)\n",
    "print(f\"Model saved to {cached_model_directory_name}/\")\n",
    "\n",
    "# To reload later:\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(cached_model_directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Fine-Tuned Model\n",
    "\n",
    "Let's see how well our model performs on the test set and compare to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(f\"  Test Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  Test Loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions\n",
    "predicted_results = trainer.predict(test_dataset)\n",
    "predicted_labels_encoded = predicted_results.predictions.argmax(-1)\n",
    "predicted_labels = [id2label[l] for l in predicted_labels_encoded]\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Error Analysis\n",
    "\n",
    "A confusion matrix shows which genres are most often confused with each other. This can reveal:\n",
    "- Which genres are easy to distinguish\n",
    "- Which genres have similar review language\n",
    "- Where the model needs improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix data\n",
    "genre_classifications_dict = defaultdict(int)\n",
    "for true_label, predicted_label in zip(test_labels, predicted_labels):\n",
    "    genre_classifications_dict[(true_label, predicted_label)] += 1\n",
    "\n",
    "# Convert to dataframe for visualization\n",
    "dicts_to_plot = []\n",
    "for (true_genre, predicted_genre), count in genre_classifications_dict.items():\n",
    "    dicts_to_plot.append({\n",
    "        'True Genre': true_genre,\n",
    "        'Predicted Genre': predicted_genre,\n",
    "        'Number of Classifications': count\n",
    "    })\n",
    "\n",
    "df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "df_wide = df_to_plot.pivot_table(\n",
    "    index='True Genre',\n",
    "    columns='Predicted Genre',\n",
    "    values='Number of Classifications'\n",
    ")\n",
    "\n",
    "# Plot full confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_wide, annot=True, fmt='g', cmap='Purples', linewidths=1)\n",
    "plt.title('Confusion Matrix: All Predictions')\n",
    "plt.xlabel('Predicted Genre')\n",
    "plt.ylabel('True Genre')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot misclassifications only (remove diagonal)\n",
    "genre_misclassifications_dict = defaultdict(int)\n",
    "for true_label, predicted_label in zip(test_labels, predicted_labels):\n",
    "    if true_label != predicted_label:\n",
    "        genre_misclassifications_dict[(true_label, predicted_label)] += 1\n",
    "\n",
    "dicts_to_plot = []\n",
    "for (true_genre, predicted_genre), count in genre_misclassifications_dict.items():\n",
    "    dicts_to_plot.append({\n",
    "        'True Genre': true_genre,\n",
    "        'Predicted Genre': predicted_genre,\n",
    "        'Number of Misclassifications': count\n",
    "    })\n",
    "\n",
    "if dicts_to_plot:\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index='True Genre',\n",
    "        columns='Predicted Genre',\n",
    "        values='Number of Misclassifications'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df_wide, annot=True, fmt='g', cmap='Reds', linewidths=1)\n",
    "    plt.title('Confusion Matrix: Misclassifications Only')\n",
    "    plt.xlabel('Predicted Genre')\n",
    "    plt.ylabel('True Genre')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Perfect classification — no misclassifications to plot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Correct and Incorrect Predictions\n",
    "\n",
    "Looking at individual examples helps us understand:\n",
    "- What the model learned\n",
    "- What kinds of errors it makes\n",
    "- Whether errors are understandable (ambiguous cases) or concerning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correctly classified examples\n",
    "print(\"CORRECTLY CLASSIFIED EXAMPLES:\\n\" + \"=\"*80)\n",
    "correct_count = 0\n",
    "for true_label, predicted_label, text in random.sample(list(zip(test_labels, predicted_labels, test_texts)), 50):\n",
    "    if true_label == predicted_label and correct_count < 10:\n",
    "        print(f\"\\nGenre: {true_label}\")\n",
    "        print(f\"Review: {text[:200]}...\")\n",
    "        correct_count += 1\n",
    "        if correct_count >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show misclassified examples\n",
    "print(\"\\n\\nMISCLASSIFIED EXAMPLES:\\n\" + \"=\"*80)\n",
    "error_count = 0\n",
    "for true_label, predicted_label, text in random.sample(list(zip(test_labels, predicted_labels, test_texts)), 50):\n",
    "    if true_label != predicted_label and error_count < 10:\n",
    "        print(f\"\\nTrue Genre: {true_label}\")\n",
    "        print(f\"Predicted Genre: {predicted_label}\")\n",
    "        print(f\"Review: {text[:200]}...\")\n",
    "        error_count += 1\n",
    "        if error_count >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Advanced Topics - FLAN-T5 Zero-Shot\n",
    "\n",
    "**Optional:** This section demonstrates advanced zero-shot classification using FLAN-T5, an encoder-decoder model fine-tuned on instruction-following tasks.\n",
    "\n",
    "Unlike the pipeline-based zero-shot classifier, FLAN-T5:\n",
    "- Uses **prompt engineering** to guide the model\n",
    "- Evaluates choices by **comparing loss values**\n",
    "- Can be more flexible but requires more code\n",
    "\n",
    "**Use cases:**\n",
    "- When you want fine control over prompts\n",
    "- When you need to classify with complex instructions\n",
    "- For research on prompt engineering\n",
    "\n",
    "**Requirements:** GPU recommended for larger FLAN-T5 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FLAN-T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FLAN-T5 (instruction-tuned encoder-decoder)\n",
    "# Options: flan-t5-small, flan-t5-base, flan-t5-large, flan-t5-xl\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "print(f'Loading {model_id} on {device}...')\n",
    "try:\n",
    "    flan_model = T5ForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "    flan_tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "    print(\"FLAN-T5 loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load FLAN-T5: {e}\")\n",
    "    print(\"Skipping advanced zero-shot section.\")\n",
    "    flan_model = None\n",
    "    flan_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Different prompts can significantly affect model performance. Let's define several templates to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prompt_1(text, possible_choices):\n",
    "    return f'Which genre of book is the following review about?\\nReview: {text}\\nChoices: {possible_choices[0]} or {possible_choices[1]}\\nAnswer:'\n",
    "\n",
    "def apply_prompt_2(text, possible_choices):\n",
    "    return f'Review: {text}\\nChoices: {possible_choices[0]} or {possible_choices[1]}\\nGenre:'\n",
    "\n",
    "def apply_prompt_3(text, possible_choices):\n",
    "    return f'Review: {text}\\nGenre:'\n",
    "\n",
    "def apply_prompt_4(text, possible_choices):\n",
    "    return f'\\nReview: {text}\\nWhich genre of book is the review about?'\n",
    "\n",
    "def apply_prompt_5(text, possible_choices):\n",
    "    return f'Review: {text}\\nChoices: {possible_choices[0]} or {possible_choices[1]}\\nAnswer:'\n",
    "\n",
    "# Test a prompt\n",
    "test_text = \"This novel has amazing world-building and magical creatures.\"\n",
    "test_choices = [\"fantasy\", \"history\"]\n",
    "print(\"Example prompt:\")\n",
    "print(apply_prompt_1(test_text, test_choices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss-by-Choice Classification\n",
    "\n",
    "This approach:\n",
    "1. For each possible choice, calculate how likely the model thinks that choice is\n",
    "2. The choice with lowest loss (highest likelihood) is the prediction\n",
    "3. Works without any training — purely zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_example(text, label, possible_choices, model, tokenizer, verbose=False):\n",
    "    \"\"\"Classify a single example using loss-by-choice.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True).to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    losses_and_targets = []\n",
    "    for target_text in possible_choices:\n",
    "        target = tokenizer(target_text, return_tensors='pt', truncation=True).to(device)\n",
    "        target_ids = target.input_ids\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "        \n",
    "        loss = outputs.loss.item()\n",
    "        losses_and_targets.append((loss, target_text))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  {target_text}: loss = {loss:.4f}\")\n",
    "    \n",
    "    losses_and_targets.sort()\n",
    "    _, best_choice = losses_and_targets[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  → Predicted: {best_choice} (True: {label})\")\n",
    "    \n",
    "    return best_choice == label\n",
    "\n",
    "def classify_dataset(prompted_examples, labels, possible_choices, model, tokenizer, verbose=False):\n",
    "    \"\"\"Classify a dataset and return accuracy.\"\"\"\n",
    "    num_examples = len(prompted_examples)\n",
    "    correct = 0\n",
    "    \n",
    "    for i in tqdm(range(num_examples), desc=\"Classifying\"):\n",
    "        prompted_example = prompted_examples[i]\n",
    "        label = labels[i]\n",
    "        is_correct = classify_example(\n",
    "            prompted_example, label, possible_choices, model, tokenizer,\n",
    "            verbose=(i < 5 and verbose)\n",
    "        )\n",
    "        correct += int(is_correct)\n",
    "    \n",
    "    return correct / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Binary Classification\n",
    "\n",
    "Let's test FLAN-T5 on a binary classification task: distinguishing history/biography from poetry reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flan_model is not None and genre_reviews_dict:\n",
    "    # Helper function to subsample two classes\n",
    "    def subsample_two_classes(all_texts, all_labels, label_1, label_2, n):\n",
    "        all_texts = np.array(all_texts)\n",
    "        all_labels = np.array(all_labels)\n",
    "        idxs_label_1 = np.where(all_labels == label_1)[0].tolist()\n",
    "        idxs_label_2 = np.where(all_labels == label_2)[0].tolist()\n",
    "        n_each_class = int(n/2)\n",
    "        idxs_label_1 = idxs_label_1[:n_each_class]\n",
    "        idxs_label_2 = idxs_label_2[:n_each_class]\n",
    "        subset_idxs = idxs_label_1 + idxs_label_2\n",
    "        random.shuffle(subset_idxs)\n",
    "        subset_texts = list(all_texts[subset_idxs])\n",
    "        subset_labels = list(all_labels[subset_idxs])\n",
    "        return subset_texts, subset_labels\n",
    "    \n",
    "    # Prepare binary classification task\n",
    "    task_texts, task_labels = subsample_two_classes(\n",
    "        test_texts, test_labels, \n",
    "        'history_biography', 'poetry', \n",
    "        n=100\n",
    "    )\n",
    "    \n",
    "    # Simplify labels for prompting\n",
    "    original_label_to_new_name = {\n",
    "        'history_biography': 'history/biography', \n",
    "        'poetry': 'poetry'\n",
    "    }\n",
    "    possible_choices = list(original_label_to_new_name.values())\n",
    "    task_labels = [original_label_to_new_name[l] for l in task_labels]\n",
    "    \n",
    "    # Apply prompt\n",
    "    task_texts_prompted = [apply_prompt_1(t, possible_choices) for t in task_texts]\n",
    "    \n",
    "    print(\"\\nExample prompted text:\")\n",
    "    print(task_texts_prompted[0][:300])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Classify\n",
    "    accuracy = classify_dataset(\n",
    "        task_texts_prompted, task_labels, possible_choices,\n",
    "        flan_model, flan_tokenizer, verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\\nZero-shot accuracy (history/biography vs poetry): {accuracy*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Skipping zero-shot example (model not loaded or data unavailable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering Experiments\n",
    "\n",
    "Try different prompts and see how they affect accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flan_model is not None and 'task_texts' in locals():\n",
    "    prompts_to_test = [\n",
    "        (\"Prompt 1: Full question\", apply_prompt_1),\n",
    "        (\"Prompt 2: Choices then genre\", apply_prompt_2),\n",
    "        (\"Prompt 3: Minimal\", apply_prompt_3),\n",
    "        (\"Prompt 5: Review choices answer\", apply_prompt_5),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for name, prompt_func in prompts_to_test:\n",
    "        print(f\"\\nTesting {name}...\")\n",
    "        prompted = [prompt_func(t, possible_choices) for t in task_texts]\n",
    "        acc = classify_dataset(prompted, task_labels, possible_choices, flan_model, flan_tokenizer)\n",
    "        results.append({'Prompt': name, 'Accuracy': acc})\n",
    "        print(f\"{name}: {acc*100:.2f}%\")\n",
    "    \n",
    "    # Show results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df.sort_values('Accuracy', ascending=False))\n",
    "else:\n",
    "    print(\"Skipping prompt experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed a comprehensive tour of transformer models:\n",
    "\n",
    "**What you learned:**\n",
    "1. Text generation with GPT-2 and parameter tuning\n",
    "2. Text embeddings and semantic similarity with BERT\n",
    "3. Pre-trained pipelines for common NLP tasks\n",
    "4. Fine-tuning BERT for custom classification\n",
    "5. Advanced zero-shot classification with FLAN-T5\n",
    "\n",
    "**Next steps:**\n",
    "- Try fine-tuning on your own dataset\n",
    "- Experiment with different model sizes\n",
    "- Explore other Hugging Face models (RoBERTa, ALBERT, DeBERTa)\n",
    "- Use embeddings for clustering and visualization\n",
    "- Build applications with the Transformers library\n",
    "\n",
    "**Resources:**\n",
    "- [Hugging Face Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [Model Hub](https://huggingface.co/models)\n",
    "- [AI for Humanists](https://aiforhumanists.com/)\n",
    "- [BERT Paper](https://arxiv.org/abs/1810.04805)\n",
    "- [GPT-2 Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
