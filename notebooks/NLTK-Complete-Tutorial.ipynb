{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Complete Tutorial\n",
    "\n",
    "**Summary:**\n",
    "This notebook offers a complete tutorial on NLTK, covering text preprocessing, tokenization, tagging, and analysis for English texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete NLTK Tutorial\n",
    "\n",
    "This comprehensive notebook covers text mining and natural language processing using NLTK (Natural Language Toolkit).\n",
    "\n",
    "## Overview\n",
    "\n",
    "NLTK is a platform for building Python programs to analyze natural language data. It provides tools for:\n",
    "- Text exploration and analysis\n",
    "- Tokenization and text processing\n",
    "- Finding collocations (word pairs)\n",
    "- Part-of-speech tagging\n",
    "- Named entity recognition\n",
    "- Text classification using machine learning\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Introduction & Setup** - Import libraries and download necessary NLTK data\n",
    "2. **Part 1: Text Exploration** - Concordance, similarity, dispersion plots, lexical diversity\n",
    "3. **Part 2: Tokenization & Text Processing** - Word and sentence tokenization\n",
    "4. **Part 3: Collocations** - Finding bigrams with PMI (Pointwise Mutual Information)\n",
    "5. **Part 4: Parts of Speech Tagging** - Extracting and visualizing nouns\n",
    "6. **Part 5: Named Entity Recognition** - Extracting people, places, and organizations\n",
    "7. **Part 6: Text Classification** - Gender classification and criminal conviction categorization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction & Setup\n",
    "\n",
    "First, we'll import all necessary libraries and download the NLTK data packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import names\n",
    "\n",
    "# Visualization imports\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all required NLTK data\n",
    "# Note: punkt_tab is required for newer versions of NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('names')\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Text Exploration\n",
    "\n",
    "NLTK ships with several built-in text corpora. Let's load them and explore basic text analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.book` module loads several classic texts:\n",
    "- text1: Moby Dick by Herman Melville\n",
    "- text2: Sense and Sensibility by Jane Austen\n",
    "- text3: The Book of Genesis\n",
    "- text4: Inaugural Address Corpus\n",
    "- text5: Chat Corpus\n",
    "- And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "\n",
    "A concordance shows every occurrence of a word along with its surrounding context. This helps understand how a word is used in different situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Words\n",
    "\n",
    "Find words that appear in similar contexts to a given word. This reveals semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the similar words differ between Moby Dick (text1) and Sense and Sensibility (text2), reflecting different writing styles and periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Contexts\n",
    "\n",
    "Find contexts shared by two or more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion Plots\n",
    "\n",
    "Visualize where words appear throughout a text. This is useful for tracking themes or concepts across a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity\n",
    "\n",
    "Lexical diversity measures how many unique words are used relative to the total word count. Higher diversity indicates richer vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    \"\"\"Calculate the ratio of unique words to total words.\"\"\"\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def percentage(count, total):\n",
    "    \"\"\"Calculate percentage.\"\"\"\n",
    "    return 100 * count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text 3 (Genesis) lexical diversity: {lexical_diversity(text3):.4f}\")\n",
    "print(f\"Text 3 length: {len(text3)} tokens\")\n",
    "print(f\"Text 3 unique words: {len(set(text3))}\")\n",
    "print()\n",
    "print(f\"Text 1 (Moby Dick) lexical diversity: {lexical_diversity(text1):.4f}\")\n",
    "print(f\"Text 1 length: {len(text1)} tokens\")\n",
    "print(f\"Text 1 unique words: {len(set(text1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Tokenization & Text Processing\n",
    "\n",
    "Tokenization is the process of dividing text into words or sentences. While Python has built-in string methods like `split()`, NLTK's tokenizers are linguistically aware and handle punctuation, abbreviations, and other edge cases intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sample Text\n",
    "\n",
    "We'll load a sample text from a URL. This is *Australian Essays* by Francis W. L. Adams (1886) from the AustLit corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch sample text from URL\n",
    "link = \"https://etc.mikelynch.org/tmfh/adaessa-plain.txt\"\n",
    "r = requests.get(link)\n",
    "sample_text = r.text\n",
    "\n",
    "# Display first 100 characters\n",
    "print(sample_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Simple vs. Smart Tokenization\n",
    "\n",
    "Let's see the difference between Python's basic `split()` and NLTK's word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple split - notice all the whitespace codes\n",
    "simple_split = sample_text.split(\" \")[:20]\n",
    "print(\"Simple split result:\")\n",
    "print(simple_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `split()` includes empty strings and whitespace characters like `\\n` and `\\t`. Now let's use NLTK's word tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "NLTK's word tokenizer understands punctuation and handles contractions, abbreviations, and other linguistic features properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = word_tokenize(sample_text)\n",
    "print(f\"Total words: {len(sample_words)}\")\n",
    "print(f\"First 20 tokens: {sample_words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "\n",
    "NLTK can also tokenize text into sentences. It's smart enough to know that periods in abbreviations like \"W.\" don't mark sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(sample_text)\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"\\nFirst 5 sentences:\")\n",
    "for i, sent in enumerate(sentences[:5], 1):\n",
    "    print(f\"\\n{i}. {sent[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Collocations\n",
    "\n",
    "Collocations are words that frequently occur together. Finding collocations can reveal phrases and concepts that are important in a text. We'll use PMI (Pointwise Mutual Information) to rank bigrams (pairs of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Bigrams (All Frequencies)\n",
    "\n",
    "First, let's look at the top bigrams using PMI scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 bigrams by PMI (all frequencies):\")\n",
    "for bigram in finder.nbest(bigram_measures.pmi, 20):\n",
    "    print(f\"  {bigram[0]} {bigram[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by Frequency\n",
    "\n",
    "Many of these bigrams only occur once or twice. Let's filter to only show bigrams that appear at least 3 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(3)\n",
    "print(\"Top 20 bigrams by PMI (frequency >= 3):\")\n",
    "for bigram in finder.nbest(bigram_measures.pmi, 20):\n",
    "    print(f\"  {bigram[0]} {bigram[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wider Context Window\n",
    "\n",
    "We can also look for words that co-occur within a larger window (not necessarily adjacent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_wide = BigramCollocationFinder.from_words(sample_words, window_size=15)\n",
    "finder_wide.apply_freq_filter(3)\n",
    "print(\"Top 20 bigrams with window size 15:\")\n",
    "for bigram in finder_wide.nbest(bigram_measures.pmi, 20):\n",
    "    print(f\"  {bigram[0]} {bigram[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cultural Note\n",
    "\n",
    "One interesting collocation in this text is \"six-toed giant.\" This phrase comes from Matthew Arnold, who popularized the pejorative use of \"philistine\" to describe people with disdain for culture. Adams dedicated his book to Arnold and used this phrase repeatedly. This shows how data analysis can reveal culturally significant patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Parts of Speech Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging assigns grammatical categories (noun, verb, adjective, etc.) to each word. This is challenging because many words can serve different roles depending on context. We'll use NLTK's Perceptron tagger to extract nouns from our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Words\n",
    "\n",
    "The `pos_tag()` function takes a list of words and returns tuples of (word, tag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = pos_tag(sample_words)\n",
    "print(\"First 30 tagged words:\")\n",
    "print(tagged_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags come from the Penn Treebank tagset:\n",
    "- NN: singular or mass noun\n",
    "- NNS: plural noun\n",
    "- NNP: proper noun, singular\n",
    "- JJ: adjective\n",
    "- VB: verb, base form\n",
    "- And many more...\n",
    "\n",
    "[Full Penn Treebank tag reference](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Nouns\n",
    "\n",
    "Let's filter the tagged words to get only nouns (NN and NNS tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nouns = [word for (word, pos) in tagged_words if pos in ['NN', 'NNS']]\n",
    "print(f\"Total nouns extracted: {len(sample_nouns)}\")\n",
    "print(f\"First 30 nouns: {sample_nouns[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Noun Frequency\n",
    "\n",
    "Now let's visualize the most common nouns in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(sample_nouns)\n",
    "freq_dist.plot(20, title=\"Frequency distribution for 20 most common nouns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud\n",
    "\n",
    "We can also create a word cloud to visualize the noun frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(max_font_size=60, colormap='hsv', background_color='white').generate(' '.join(sample_nouns))\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Noun Word Cloud', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) extracts and classifies named entities in text, such as:\n",
    "- PERSON: People's names\n",
    "- GPE: Geo-political entities (countries, cities, states)\n",
    "- ORGANIZATION: Companies, institutions, agencies\n",
    "- And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    \"\"\"Extract and display named entities from text.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    all_entities = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        tagged = pos_tag(tokens)\n",
    "        tree = nltk.ne_chunk(tagged)\n",
    "        \n",
    "        for subtree in tree:\n",
    "            if hasattr(subtree, 'label'):\n",
    "                entity = ' '.join(word for word, tag in subtree.leaves())\n",
    "                entity_type = subtree.label()\n",
    "                all_entities.append((entity_type, entity))\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ner = \"Barack Obama was born in Hawaii. He served as President of the United States. He visited the United Nations in New York.\"\n",
    "\n",
    "entities = extract_entities(sample_ner)\n",
    "print(\"Extracted entities:\")\n",
    "for entity_type, entity in entities:\n",
    "    print(f\"  {entity_type}: {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also extract entities from our Australian Essays text (just the first 10 sentences to keep output manageable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first 10 sentences\n",
    "first_sentences = ' '.join(sentences[:10])\n",
    "entities_essays = extract_entities(first_sentences)\n",
    "\n",
    "print(\"Entities from Australian Essays (first 10 sentences):\")\n",
    "for entity_type, entity in entities_essays[:20]:  # Show first 20 entities\n",
    "    print(f\"  {entity_type}: {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Text Classification\n",
    "\n",
    "Text classification uses machine learning to categorize documents. We'll explore two classification examples:\n",
    "1. **Gender Classification** - Predicting gender from names\n",
    "2. **Criminal Conviction Classification** - Categorizing historical criminal records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Gender Classification from Names\n",
    "\n",
    "We'll build a simple Naive Bayes classifier that predicts gender based on the last letter of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load name corpus\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "print(f\"Total names: {len(labeled_names)}\")\n",
    "print(f\"Sample names: {labeled_names[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Extractor\n",
    "\n",
    "Our feature extractor is simple - it just looks at the last letter of the name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "# Test it\n",
    "print(\"Feature examples:\")\n",
    "print(f\"  John: {gender_features('John')}\")\n",
    "print(f\"  Mary: {gender_features('Mary')}\")\n",
    "print(f\"  Alex: {gender_features('Alex')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Classifier\n",
    "\n",
    "We split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "gender_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = ['Neo', 'Trinity', 'Alex', 'Sarah', 'Pat', 'Morgan', 'Taylor', 'Jordan']\n",
    "\n",
    "print(\"Gender predictions:\")\n",
    "for name in test_names:\n",
    "    prediction = gender_classifier.classify(gender_features(name))\n",
    "    print(f\"  {name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = nltk.classify.accuracy(gender_classifier, test_set)\n",
    "print(f\"Classifier accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Informative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 2: Criminal Conviction Classification\n",
    "\n",
    "Now let's tackle a more complex classification problem. We'll use historical records of women's criminal convictions from 19th and early 20th century Australia (from Dr. Alana Piper's [Criminal Characters](https://criminalcharacters.com/) research project).\n",
    "\n",
    "We'll train a classifier to categorize convictions into three types:\n",
    "- **property** - Crimes involving theft or property damage\n",
    "- **violent** - Violent crimes and assaults\n",
    "- **nonviolent** - Other crimes like vagrancy, drunkenness, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Conviction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full corpus of convictions\n",
    "r = requests.get(\"https://etc.mikelynch.org/tmfh/cc/convictions.txt\")\n",
    "corpus = r.text\n",
    "convictions = corpus.splitlines()\n",
    "\n",
    "print(f\"Total convictions: {len(convictions)}\")\n",
    "print(\"\\nFirst 5 convictions:\")\n",
    "for i, conv in enumerate(convictions[:5], 1):\n",
    "    print(f\"{i}. {conv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Extractor\n",
    "\n",
    "For this classifier, we'll use the presence/absence of the top 100 most common words as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 100 words from entire corpus\n",
    "all_words = nltk.FreqDist(word_tokenize(corpus))\n",
    "top_words = sorted(all_words.keys(), key=lambda x: all_words[x], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 20 words in corpus:\")\n",
    "print(top_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conviction_features(conviction):\n",
    "    \"\"\"Extract features from a conviction record.\"\"\"\n",
    "    conviction_words = set(word_tokenize(conviction))\n",
    "    features = {}\n",
    "    for word in top_words:\n",
    "        features[f'contains({word})'] = (word in conviction_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the feature extractor\n",
    "sample_conviction = '03-MAY-1875 ABBOT, NORAH: 3 MONTHS IMP VAGRANCY MARYBOROUGH PETTY SESSIONS'\n",
    "features = conviction_features(sample_conviction)\n",
    "\n",
    "# Show some of the features\n",
    "print(\"Sample features (showing True values):\")\n",
    "for key, value in list(features.items())[:20]:\n",
    "    if value:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data\n",
    "\n",
    "The training data is a CSV file with 200 manually classified convictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "with requests.get(\"https://etc.mikelynch.org/tmfh/cc/training.csv\", stream=True) as r:\n",
    "    lines = (line.decode('utf-8-sig') for line in r.iter_lines())\n",
    "    for row in csv.reader(lines):\n",
    "        features = conviction_features(row[0])\n",
    "        label = row[1]\n",
    "        training_data.append((features, label))\n",
    "\n",
    "print(f\"Training data loaded: {len(training_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Sets\n",
    "\n",
    "We'll use the first 100 records to test accuracy and the remaining 100 to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_data = training_data[:100]\n",
    "training_data = training_data[100:]\n",
    "\n",
    "print(f\"Training set: {len(training_data)} records\")\n",
    "print(f\"Test set: {len(checking_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction_classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "print(\"Classifier trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Informative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction_classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for features, label in checking_data:\n",
    "    bayes_result = conviction_classifier.classify(features)\n",
    "    if bayes_result == label:\n",
    "        count += 1\n",
    "\n",
    "accuracy = count / len(checking_data)\n",
    "print(f\"Classifier accuracy: {accuracy:.2%}\")\n",
    "print(f\"Correct predictions: {count} out of {len(checking_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Sample Convictions\n",
    "\n",
    "Let's see how the classifier performs on real convictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification results for first 15 convictions:\\n\")\n",
    "for i, conv in enumerate(convictions[:15], 1):\n",
    "    label = conviction_classifier.classify(conviction_features(conv))\n",
    "    print(f\"{i}. [{label.upper()}]\")\n",
    "    print(f\"   {conv}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Entire Corpus\n",
    "\n",
    "Finally, let's classify all convictions and see the breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakdown = {}\n",
    "\n",
    "for conv in convictions:\n",
    "    label = conviction_classifier.classify(conviction_features(conv))\n",
    "    if label not in breakdown:\n",
    "        breakdown[label] = [conv]\n",
    "    else:\n",
    "        breakdown[label].append(conv)\n",
    "\n",
    "print(\"Classification breakdown:\\n\")\n",
    "for label, convs in sorted(breakdown.items()):\n",
    "    print(f\"{label.upper()}: {len(convs)} convictions ({len(convs)/len(convictions)*100:.1f}%)\")\n",
    "    print(f\"Sample convictions:\")\n",
    "    for conv in convs[:3]:\n",
    "        print(f\"  - {conv}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Classifier\n",
    "\n",
    "The 85% accuracy is reasonable for such a small training set, but could be improved by:\n",
    "\n",
    "1. **Larger training set** - More examples would help the classifier learn better patterns\n",
    "2. **Better features** - We could filter out common names, or use bigrams/trigrams\n",
    "3. **Feature engineering** - Extract specific crime-related keywords rather than just top words\n",
    "4. **Try different algorithms** - Other classifiers like Maximum Entropy might perform better\n",
    "\n",
    "The exploratory process of refining features and testing accuracy is key to building effective classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "This notebook has covered the core capabilities of NLTK:\n",
    "\n",
    "1. **Text Exploration** - Understanding how words are used in context\n",
    "2. **Tokenization** - Breaking text into meaningful units\n",
    "3. **Collocations** - Finding phrases and word associations\n",
    "4. **POS Tagging** - Identifying grammatical roles\n",
    "5. **Named Entity Recognition** - Extracting people, places, and organizations\n",
    "6. **Text Classification** - Building machine learning models to categorize text\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [NLTK Book - Natural Language Processing with Python](https://www.nltk.org/book/)\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Library Carpentry: Text & Data Mining](http://librarycarpentry.org/lc-tdm/)\n",
    "- [Criminal Characters Research Project](https://criminalcharacters.com/)\n",
    "\n",
    "## Credits\n",
    "\n",
    "This notebook combines material from:\n",
    "- Mike Lynch's \"Text Mining for the Humanities\" workshop (UTS eResearch)\n",
    "- NLTK Book examples\n",
    "- Library Carpentry resources\n",
    "\n",
    "Sample data:\n",
    "- *Australian Essays* by Francis W. L. Adams (1886) from [AustLit corpus](https://www.ausnc.org.au/corpora/austlit)\n",
    "- Criminal conviction records from Dr. Alana Piper's [Criminal Characters](https://criminalcharacters.com/) project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
