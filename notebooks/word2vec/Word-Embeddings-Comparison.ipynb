{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: Classical vs. Contextual\n",
    "\n",
    "**An introductory guide to understanding different types of word embeddings**\n",
    "\n",
    "This notebook will help you understand:\n",
    "- What word embeddings are and why they matter\n",
    "- The difference between classical (Word2Vec) and contextual (BERT) embeddings\n",
    "- When to use each approach\n",
    "- Where to go for deeper dives into each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Word Embeddings\n",
    "\n",
    "### What are Word Embeddings?\n",
    "\n",
    "Word embeddings are **numerical representations of words**. Instead of treating words as discrete symbols, we represent them as vectors (lists of numbers) that capture their meaning.\n",
    "\n",
    "### Why Do We Need Vector Representations?\n",
    "\n",
    "Computers can't understand text directly. We need to convert words into numbers that:\n",
    "1. **Capture meaning**: Similar words should have similar vectors\n",
    "2. **Enable computation**: We can measure similarity, perform arithmetic, and use them in machine learning models\n",
    "3. **Are efficient**: Better than older methods like one-hot encoding\n",
    "\n",
    "### Historical Evolution\n",
    "\n",
    "```\n",
    "One-Hot Encoding (1950s-2000s)\n",
    "    ↓\n",
    "    \"cat\" = [1, 0, 0, 0, ...]\n",
    "    \"dog\" = [0, 1, 0, 0, ...]\n",
    "    Problem: No notion of similarity!\n",
    "\n",
    "Word2Vec (2013)\n",
    "    ↓\n",
    "    \"cat\" = [0.2, -0.5, 0.8, ...]\n",
    "    \"dog\" = [0.3, -0.4, 0.7, ...]\n",
    "    Better: Similar words have similar vectors!\n",
    "    Problem: Same vector for all contexts\n",
    "\n",
    "BERT (2018)\n",
    "    ↓\n",
    "    \"bank\" in \"river bank\" = [0.1, 0.9, -0.3, ...]\n",
    "    \"bank\" in \"savings bank\" = [0.8, -0.2, 0.5, ...]\n",
    "    Best: Different vectors depending on context!\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Semantic Similarity**: Words with similar meanings have similar vectors\n",
    "- **Vector Arithmetic**: We can do math with words (\"king\" - \"man\" + \"woman\" ≈ \"queen\")\n",
    "- **Dimensionality**: Embeddings typically have 100-768 dimensions\n",
    "- **Context**: Whether the meaning changes based on surrounding words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classical Embeddings (Word2Vec)\n",
    "\n",
    "### How Word2Vec Works (High-Level)\n",
    "\n",
    "Word2Vec learns embeddings by predicting words from their context:\n",
    "\n",
    "**Two approaches:**\n",
    "1. **CBOW (Continuous Bag of Words)**: Predict center word from surrounding words\n",
    "   - Input: \"The cat sat on the\" → Output: \"mat\"\n",
    "\n",
    "2. **Skip-gram**: Predict surrounding words from center word\n",
    "   - Input: \"cat\" → Output: \"The\", \"sat\", \"on\"\n",
    "\n",
    "### Key Feature: Static Embeddings\n",
    "\n",
    "**One vector per word**, regardless of context:\n",
    "- \"bank\" always gets the same vector\n",
    "- \"bank\" in \"river bank\" = same as \"bank\" in \"savings bank\"\n",
    "- This is both a strength (simple, fast) and limitation (misses context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install gensim numpy matplotlib scikit-learn transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Word2Vec Demo\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Word2Vec model (this may take a minute)\n",
    "print(\"Loading Word2Vec model...\")\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Find similar words\n",
    "print(\"Words most similar to 'cat':\")\n",
    "for word, similarity in word2vec_model.most_similar('cat', topn=5):\n",
    "    print(f\"  {word}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Word arithmetic (the famous king - man + woman = queen)\n",
    "result = word2vec_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=3)\n",
    "print(\"king - man + woman =\")\n",
    "for word, similarity in result:\n",
    "    print(f\"  {word}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: The \"bank\" problem\n",
    "# Word2Vec gives the same vector for \"bank\" regardless of context\n",
    "bank_vector = word2vec_model['bank']\n",
    "print(f\"Vector for 'bank': {bank_vector[:5]}...  (showing first 5 of 300 dimensions)\")\n",
    "print(f\"\\nThis is the SAME vector whether we're talking about:\")\n",
    "print(\"  - A river bank\")\n",
    "print(\"  - A savings bank\")\n",
    "print(\"  - A blood bank\")\n",
    "print(\"\\nThis is a limitation of static embeddings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Simple t-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select some words to visualize\n",
    "words = ['cat', 'dog', 'kitten', 'puppy', 'animal', 'pet',\n",
    "         'king', 'queen', 'prince', 'princess', 'royal', 'crown',\n",
    "         'bank', 'river', 'water', 'money', 'finance', 'loan']\n",
    "\n",
    "# Get vectors for these words\n",
    "vectors = [word2vec_model[word] for word in words]\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.5)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                fontsize=12, alpha=0.8)\n",
    "\n",
    "plt.title('Word2Vec Embeddings Visualized (t-SNE)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how semantically related words cluster together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to Learn More About Word2Vec?\n",
    "\n",
    "See **[Word2Vec.ipynb](./Word2Vec.ipynb)** for a comprehensive deep dive including:\n",
    "- Training your own Word2Vec model from scratch\n",
    "- Advanced visualization techniques\n",
    "- Using Word2Vec for text classification\n",
    "- Document embeddings\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Contextual Embeddings (BERT)\n",
    "\n",
    "### How BERT Embeddings Differ\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) creates **context-dependent embeddings**:\n",
    "\n",
    "- The same word gets **different vectors** in different contexts\n",
    "- BERT looks at the entire sentence to understand each word\n",
    "- More computationally expensive but much more powerful\n",
    "\n",
    "### The \"Bank\" Example Solved\n",
    "\n",
    "```\n",
    "Word2Vec:\n",
    "  \"bank\" → [0.1, 0.5, -0.3, ...]  (always the same)\n",
    "\n",
    "BERT:\n",
    "  \"I sat by the river bank\"  → \"bank\" = [0.2, 0.8, -0.1, ...]\n",
    "  \"I deposited money at the bank\" → \"bank\" = [0.7, -0.3, 0.5, ...]\n",
    "  \n",
    "Different contexts → Different embeddings!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple BERT Demo\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "print(\"Loading BERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get BERT embedding for a word in context\n",
    "def get_word_embedding(sentence, word):\n",
    "    \"\"\"Get BERT embedding for a specific word in a sentence.\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Get word position\n",
    "    word_idx = tokens.index(word) + 1  # +1 for [CLS] token\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embedding for the specific word\n",
    "    word_embedding = outputs.last_hidden_state[0, word_idx, :]\n",
    "    return word_embedding.numpy()\n",
    "\n",
    "# Example: \"bank\" in different contexts\n",
    "sentence1 = \"I sat by the river bank\"\n",
    "sentence2 = \"I deposited money at the bank\"\n",
    "\n",
    "embedding1 = get_word_embedding(sentence1, 'bank')\n",
    "embedding2 = get_word_embedding(sentence2, 'bank')\n",
    "\n",
    "print(f\"Embedding for 'bank' in '{sentence1}':\")\n",
    "print(f\"  {embedding1[:5]}... (showing first 5 of 768 dimensions)\\n\")\n",
    "\n",
    "print(f\"Embedding for 'bank' in '{sentence2}':\")\n",
    "print(f\"  {embedding2[:5]}... (showing first 5 of 768 dimensions)\\n\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "from numpy.linalg import norm\n",
    "similarity = np.dot(embedding1, embedding2) / (norm(embedding1) * norm(embedding2))\n",
    "print(f\"Similarity between the two 'bank' embeddings: {similarity:.3f}\")\n",
    "print(\"Note: They're somewhat similar but NOT identical!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sentence-level embeddings\n",
    "def get_sentence_embedding(sentence):\n",
    "    \"\"\"Get BERT embedding for entire sentence using [CLS] token.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use [CLS] token (first token) as sentence representation\n",
    "    return outputs.last_hidden_state[0, 0, :].numpy()\n",
    "\n",
    "# Compare sentence similarity\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The kitten rested on the rug\",\n",
    "    \"I need to deposit money at the bank\"\n",
    "]\n",
    "\n",
    "embeddings = [get_sentence_embedding(s) for s in sentences]\n",
    "\n",
    "print(\"Sentence similarity matrix:\\n\")\n",
    "for i, sent1 in enumerate(sentences):\n",
    "    for j, sent2 in enumerate(sentences):\n",
    "        if i <= j:  # Only show upper triangle\n",
    "            sim = np.dot(embeddings[i], embeddings[j]) / (norm(embeddings[i]) * norm(embeddings[j]))\n",
    "            print(f\"'{sent1}' <-> '{sent2}'\")\n",
    "            print(f\"  Similarity: {sim:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to Learn More About BERT?\n",
    "\n",
    "Check out these notebooks:\n",
    "- **[BERT-For-Humanists_Word-Similarity_English-Public-Domain-Poetry.ipynb](./BERT-For-Humanists_Word-Similarity_English-Public-Domain-Poetry.ipynb)** - Word similarity analysis with BERT on poetry\n",
    "- **[Transformers-Complete-Guide.ipynb](../transformers/Transformers-Complete-Guide.ipynb)** - Comprehensive guide to transformer models including BERT, GPT, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Side-by-Side Comparison\n",
    "\n",
    "| Feature | Word2Vec (Classical) | BERT (Contextual) |\n",
    "|---------|---------------------|-------------------|\n",
    "| **Embedding Type** | Static | Dynamic/Contextual |\n",
    "| **Context Awareness** | No - same vector always | Yes - changes with context |\n",
    "| **Vector per Word** | One | Many (depends on context) |\n",
    "| **Dimensions** | 100-300 | 768-1024 |\n",
    "| **Speed** | Fast | Slower |\n",
    "| **Memory** | Low | High |\n",
    "| **Training** | Unsupervised on corpus | Pre-trained on massive data |\n",
    "| **Word Arithmetic** | Yes (king - man + woman) | Not directly |\n",
    "| **Sentence Embeddings** | By averaging words | Native support |\n",
    "| **Best For** | Large vocabularies, speed | Ambiguous words, context |\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Word2Vec: One Representation\n",
    "┌─────────────────────────────────┐\n",
    "│ \"bank\" → [0.1, 0.5, -0.3, ...]  │\n",
    "└─────────────────────────────────┘\n",
    "        ↓         ↓         ↓\n",
    "   river bank | savings bank | blood bank\n",
    "   (all use the same vector)\n",
    "\n",
    "BERT: Context-Dependent\n",
    "┌────────────────────────────────────────────┐\n",
    "│ \"I sat by the river bank\"                  │\n",
    "│    → \"bank\" = [0.2, 0.8, -0.1, ...]        │\n",
    "├────────────────────────────────────────────┤\n",
    "│ \"I deposited money at the bank\"            │\n",
    "│    → \"bank\" = [0.7, -0.3, 0.5, ...]        │\n",
    "├────────────────────────────────────────────┤\n",
    "│ \"The blood bank needs donors\"              │\n",
    "│    → \"bank\" = [0.4, 0.1, 0.9, ...]         │\n",
    "└────────────────────────────────────────────┘\n",
    "(different vectors for different contexts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct comparison: Word2Vec vs BERT for polysemous words\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARING Word2Vec vs BERT: The 'bank' example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Word2Vec: Always the same\n",
    "print(\"\\n1. WORD2VEC (Static Embedding)\")\n",
    "print(\"-\" * 60)\n",
    "w2v_bank = word2vec_model['bank']\n",
    "print(f\"'bank' vector (always): {w2v_bank[:5]}...\")\n",
    "print(\"\\n  Used in: 'river bank', 'savings bank', 'blood bank'\")\n",
    "print(\"  Result: IDENTICAL vector in all contexts\")\n",
    "\n",
    "# BERT: Context-dependent\n",
    "print(\"\\n2. BERT (Contextual Embedding)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "contexts = [\n",
    "    \"I sat by the river bank\",\n",
    "    \"I deposited money at the bank\",\n",
    "    \"The blood bank needs donors\"\n",
    "]\n",
    "\n",
    "bert_embeddings = []\n",
    "for ctx in contexts:\n",
    "    emb = get_word_embedding(ctx, 'bank')\n",
    "    bert_embeddings.append(emb)\n",
    "    print(f\"\\n  '{ctx}'\")\n",
    "    print(f\"  'bank' vector: {emb[:5]}...\")\n",
    "\n",
    "# Calculate pairwise similarities for BERT embeddings\n",
    "print(\"\\n3. BERT Embedding Similarities\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(len(contexts)):\n",
    "    for j in range(i + 1, len(contexts)):\n",
    "        sim = np.dot(bert_embeddings[i], bert_embeddings[j]) / \\\n",
    "              (norm(bert_embeddings[i]) * norm(bert_embeddings[j]))\n",
    "        print(f\"\\n  Context {i+1} vs Context {j+1}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT: BERT creates different embeddings for 'bank'\")\n",
    "print(\"based on context, while Word2Vec always uses the same one.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Decision Guide\n",
    "\n",
    "### Use Word2Vec When:\n",
    "\n",
    "1. **You need fast embeddings**\n",
    "   - Real-time applications\n",
    "   - Processing large volumes of text\n",
    "   - Limited computational resources\n",
    "\n",
    "2. **Your vocabulary is fixed**\n",
    "   - Domain-specific terminology\n",
    "   - You're working with a specialized corpus\n",
    "\n",
    "3. **You're working with large corpora**\n",
    "   - Training on millions of documents\n",
    "   - News articles, books, Wikipedia\n",
    "\n",
    "4. **You need word arithmetic**\n",
    "   - Analogies (king - man + woman = queen)\n",
    "   - Exploring semantic relationships\n",
    "   - Word algebra operations\n",
    "\n",
    "5. **Simple similarity is sufficient**\n",
    "   - Finding related words\n",
    "   - Basic semantic search\n",
    "   - Document clustering\n",
    "\n",
    "### Use BERT When:\n",
    "\n",
    "1. **Context matters**\n",
    "   - Polysemous words (multiple meanings)\n",
    "   - Disambiguating word sense\n",
    "   - Understanding nuanced text\n",
    "\n",
    "2. **You need sentence-level representations**\n",
    "   - Semantic similarity between sentences\n",
    "   - Text classification\n",
    "   - Question answering\n",
    "\n",
    "3. **You have computational resources**\n",
    "   - GPU available\n",
    "   - Can handle longer processing times\n",
    "   - Memory is not a constraint\n",
    "\n",
    "4. **You're fine-tuning for specific tasks**\n",
    "   - Named entity recognition\n",
    "   - Sentiment analysis\n",
    "   - Custom classification tasks\n",
    "\n",
    "5. **State-of-the-art performance is required**\n",
    "   - Production systems\n",
    "   - Research applications\n",
    "   - Competitive benchmarks\n",
    "\n",
    "### Rule of Thumb:\n",
    "\n",
    "```\n",
    "Start with Word2Vec if:\n",
    "  - You're exploring data\n",
    "  - Speed matters more than accuracy\n",
    "  - Your use case is straightforward\n",
    "\n",
    "Move to BERT if:\n",
    "  - Word2Vec isn't accurate enough\n",
    "  - Context is crucial for your task\n",
    "  - You need state-of-the-art results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Next Steps\n",
    "\n",
    "### Deep Dive Notebooks\n",
    "\n",
    "1. **[Word2Vec.ipynb](./Word2Vec.ipynb)**\n",
    "   - Complete guide to Word2Vec\n",
    "   - Training your own models\n",
    "   - Advanced techniques and optimizations\n",
    "   - Document embeddings with Doc2Vec\n",
    "   - Visualization and analysis\n",
    "\n",
    "2. **[BERT-For-Humanists_Word-Similarity_English-Public-Domain-Poetry.ipynb](./BERT-For-Humanists_Word-Similarity_English-Public-Domain-Poetry.ipynb)**\n",
    "   - BERT word similarity on literary texts\n",
    "   - Working with poetry and historical texts\n",
    "   - Practical examples for humanities research\n",
    "\n",
    "3. **[Transformers-Complete-Guide.ipynb](../transformers/Transformers-Complete-Guide.ipynb)**\n",
    "   - Comprehensive transformer tutorial\n",
    "   - BERT, GPT, T5, and more\n",
    "   - Fine-tuning for custom tasks\n",
    "   - Advanced transformer techniques\n",
    "\n",
    "### Related Topics\n",
    "\n",
    "- **Topic Modeling**: Use embeddings for better topic discovery\n",
    "- **Text Classification**: Leverage embeddings as features\n",
    "- **Semantic Search**: Build search engines with embeddings\n",
    "- **Information Retrieval**: Find similar documents efficiently\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "```\n",
    "Beginner:\n",
    "  1. This notebook (concepts and basics)\n",
    "  2. Word2Vec.ipynb (classical embeddings)\n",
    "  3. BERT-For-Humanists (simple BERT examples)\n",
    "\n",
    "Intermediate:\n",
    "  4. Transformers-Complete-Guide.ipynb\n",
    "  5. Apply embeddings to your specific domain\n",
    "  6. Experiment with fine-tuning\n",
    "\n",
    "Advanced:\n",
    "  7. Train custom embeddings on your corpus\n",
    "  8. Implement hybrid approaches\n",
    "  9. Optimize for production use\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Word embeddings convert text to numbers** that capture meaning\n",
    "\n",
    "2. **Word2Vec (classical)**:\n",
    "   - Fast and efficient\n",
    "   - One vector per word\n",
    "   - Great for exploration and simple tasks\n",
    "   - Limited by lack of context\n",
    "\n",
    "3. **BERT (contextual)**:\n",
    "   - Slower but more accurate\n",
    "   - Different vectors for different contexts\n",
    "   - Handles ambiguity well\n",
    "   - Requires more resources\n",
    "\n",
    "4. **Choose based on your needs**:\n",
    "   - Speed vs. accuracy trade-off\n",
    "   - Context importance\n",
    "   - Available computational resources\n",
    "\n",
    "5. **Both have their place**:\n",
    "   - Word2Vec isn't obsolete\n",
    "   - BERT isn't always necessary\n",
    "   - Use the right tool for the job\n",
    "\n",
    "### Where to Go From Here\n",
    "\n",
    "Start with the notebook that matches your immediate need:\n",
    "- Need fast, simple embeddings? → **Word2Vec.ipynb**\n",
    "- Working with ambiguous text? → **BERT-For-Humanists**\n",
    "- Want comprehensive knowledge? → **Transformers-Complete-Guide.ipynb**\n",
    "\n",
    "Happy embedding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
