{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Hugging Face Transformers\n",
    "\n",
    "This notebook covers core transformer tasks using the [Hugging Face Transformers](https://huggingface.co/docs/transformers/) library:\n",
    "\n",
    "1. **Text Generation** — Generate text with GPT-2\n",
    "2. **Text Embeddings** — Extract BERT embeddings and measure semantic similarity\n",
    "3. **Sentiment Analysis** — Classify text with a pretrained BERT pipeline\n",
    "4. **Fine-Tuning** — Fine-tune GPT-2 on your own text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Text Generation with GPT-2\n",
    "\n",
    "GPT-2 is an autoregressive language model that generates text by predicting the next token.\n",
    "\n",
    "| Model | Parameters | HF Name |\n",
    "|-------|-----------|----------|\n",
    "| Small | 124M | `gpt2` |\n",
    "| Medium | 355M | `gpt2-medium` |\n",
    "| Large | 774M | `gpt2-large` |\n",
    "| XL | 1.5B | `gpt2-xl` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "print(f\"Loaded gpt2 ({gpt2_model.num_parameters():,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7,\n",
    "             top_k=50, top_p=0.9, num_samples=1):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_samples,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    for i, output in enumerate(outputs):\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        if num_samples > 1:\n",
    "            print(f\"--- Sample {i + 1} ---\")\n",
    "        print(text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(gpt2_model, gpt2_tokenizer, \"The secret of life is\", max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Parameters\n",
    "\n",
    "- **temperature**: Higher = more creative/random (0.3 for focused, 1.0 for wild)\n",
    "- **top_k**: Only consider the top k most likely next tokens\n",
    "- **top_p**: Nucleus sampling — only consider tokens whose cumulative probability reaches p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(gpt2_model, gpt2_tokenizer, \"Once upon a time\",\n",
    "         max_new_tokens=80, temperature=0.9, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Text Embeddings with BERT\n",
    "\n",
    "BERT produces contextual embeddings — vector representations where meaning depends on surrounding context. These embeddings power similarity search, clustering, and classification.\n",
    "\n",
    "We use the `[CLS]` token's embedding as a representation of the entire input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "print(f\"Loaded bert-base-uncased ({bert_model.num_parameters():,} parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"Get [CLS] token embeddings for a list of texts.\"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # [CLS] token is at position 0\n",
    "    return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A kitten rested on the rug.\",\n",
    "    \"Stock prices rose sharply today.\",\n",
    "    \"The financial markets surged.\",\n",
    "]\n",
    "\n",
    "embeddings = get_embeddings(sentences, bert_tokenizer, bert_model)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"(batch_size={embeddings.shape[0]}, hidden_size={embeddings.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "Cosine similarity between embeddings measures how semantically similar two sentences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "n = len(sentences)\n",
    "sim_matrix = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sim_matrix[i, j] = cosine_similarity(embeddings[i].unsqueeze(0), embeddings[j].unsqueeze(0))\n",
    "\n",
    "labels = [s[:30] + \"...\" if len(s) > 30 else s for s in sentences]\n",
    "sim_df = pd.DataFrame(sim_matrix.numpy(), index=labels, columns=labels)\n",
    "sim_df.style.background_gradient(cmap=\"YlOrRd\", vmin=0.8, vmax=1.0).format(\"{:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Sentiment Analysis\n",
    "\n",
    "Hugging Face `pipeline` provides a high-level API for common tasks. The sentiment analysis pipeline uses a BERT model fine-tuned on movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "reviews = [\n",
    "    \"This movie was absolutely wonderful! The acting was superb.\",\n",
    "    \"Terrible film. I walked out after 30 minutes.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"A masterpiece of modern cinema. Truly breathtaking.\",\n",
    "    \"The plot made no sense and the dialogue was awful.\",\n",
    "]\n",
    "\n",
    "results = sentiment(reviews)\n",
    "\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"{result['label']:8} ({result['score']:.3f})  {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pipelines\n",
    "\n",
    "Hugging Face provides pipelines for many tasks. Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "entities = ner(\"Barack Obama graduated from Harvard Law School and served as President of the United States.\")\n",
    "\n",
    "for ent in entities:\n",
    "    print(f\"{ent['entity_group']:10} {ent['word']:20} (score: {ent['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification — classify text without training\n",
    "classifier = pipeline(\"zero-shot-classification\", device=device)\n",
    "\n",
    "result = classifier(\n",
    "    \"The new iPhone features a faster processor and improved camera system.\",\n",
    "    candidate_labels=[\"technology\", \"politics\", \"sports\", \"science\"]\n",
    ")\n",
    "\n",
    "for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "    print(f\"{label:15} {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Fine-Tune GPT-2 on Custom Text\n",
    "\n",
    "Fine-tuning adapts the pretrained model to generate text in the style of your dataset.\n",
    "\n",
    "Set `TRAIN_FILE` to the path of a `.txt` file you want to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"train.txt\"  # path to your text file\n",
    "OUTPUT_DIR = \"gpt2-finetuned\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "BLOCK_SIZE = 128  # sequence length for training chunks\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Tokenize a text file and split it into fixed-length chunks for training.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, block_size):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        tokens = tokenizer.encode(text)\n",
    "        self.examples = [\n",
    "            torch.tensor(tokens[i : i + block_size])\n",
    "            for i in range(0, len(tokens) - block_size, block_size)\n",
    "        ]\n",
    "        print(f\"Loaded {len(tokens):,} tokens -> {len(self.examples)} training chunks\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(TRAIN_FILE, gpt2_tokenizer, BLOCK_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(gpt2_model.parameters(), lr=LEARNING_RATE)\n",
    "gpt2_model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        outputs = gpt2_model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch + 1}, Step {step + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} complete. Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "gpt2_model.save_pretrained(OUTPUT_DIR)\n",
    "gpt2_tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR).to(device)\n",
    "ft_tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
    "ft_model.eval()\n",
    "\n",
    "generate(ft_model, ft_tokenizer, \"The\", max_new_tokens=100, temperature=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
